ETL and Data Warehousing Project
This repository contains a Python-based ETL (Extract, Transform, Load) project designed to build a simple data warehouse. The project demonstrates the process of ingesting raw data from CSV files, cleaning and transforming it using Python, and loading it into a SQLite database with a star schema for efficient data analysis.

Project Description
The goal of this project is to simulate a business intelligence workflow. It takes transactional sales data and turns it into a structured database optimized for reporting and analytics. The script performs the following steps:

Extract: Reads raw sales and product data from CSV files.

Transform:

Cleans and prepares the data.

Calculates new metrics, such as the total sale amount for each transaction.

Splits the data into different tables based on a star schema design (fact and dimension tables).

Load: Creates a SQLite database and loads the transformed data into the defined tables.

Star Schema
The project uses a star schema, which is a common data warehousing model.

Fact Table: sales_facts - Contains the core, quantitative data and foreign keys linking to the dimension tables.

Dimension Tables:

products_dim - Contains descriptive information about products.

dates_dim - Contains descriptive information about the date of each sale.

How to Run the Project
Prerequisites: Ensure you have Python installed. The project requires the pandas library.

pip install pandas

Files: Place the Python script and your CSV data files in the same directory.

Execution: Run the script from your terminal.

python your_script_name.py

This will generate a new file named sales_data.db containing your data warehouse.

Project Structure
your_script_name.py: The main Python script that performs the ETL process.

sales.csv: Simulated transactional sales data.

products.csv: Simulated product information.

sales_data.db: The SQLite database generated by the script.

Next Steps
This is a foundational project. It can be extended by:

Adding more data sources (e.g., customer data).

Implementing a more robust error-handling mechanism.

Connecting to a different database (e.g., PostgreSQL or MySQL).

Using a dedicated ETL framework like Apache Airflow or Prefect for orchestration.
